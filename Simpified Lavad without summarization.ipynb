{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "460521ba-e3b3-4c13-a8ef-e275277df49b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting opencv-python-headless\n",
      "  Using cached opencv_python_headless-4.10.0.84-cp37-abi3-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (29.3 MB)\n",
      "Collecting transformers\n",
      "  Using cached transformers-4.47.0-py3-none-any.whl (10.1 MB)\n",
      "Requirement already satisfied: torch in /usr/lib/python3/dist-packages (2.4.1)\n",
      "Requirement already satisfied: numpy>=1.19.3 in /usr/lib/python3/dist-packages (from opencv-python-headless) (1.21.5)\n",
      "Requirement already satisfied: requests in /usr/lib/python3/dist-packages (from transformers) (2.25.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/lib/python3/dist-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: filelock in /usr/lib/python3/dist-packages (from transformers) (3.6.0)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in ./.local/lib/python3.10/site-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./.local/lib/python3.10/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in ./.local/lib/python3.10/site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/lib/python3/dist-packages (from transformers) (5.4.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in ./.local/lib/python3.10/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in ./.local/lib/python3.10/site-packages (from transformers) (0.26.5)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/lib/python3/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (2024.3.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/lib/python3/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.9.0)\n",
      "Installing collected packages: opencv-python-headless, transformers\n",
      "Successfully installed opencv-python-headless-4.10.0.84 transformers-4.47.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting faiss-cpu\n",
      "  Using cached faiss_cpu-1.9.0.post1-cp310-cp310-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (3.6 MB)\n",
      "Collecting numpy<3.0,>=1.25.0\n",
      "  Using cached numpy-2.2.0-cp310-cp310-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (14.3 MB)\n",
      "Requirement already satisfied: packaging in /usr/lib/python3/dist-packages (from faiss-cpu) (21.3)\n",
      "Installing collected packages: numpy, faiss-cpu\n",
      "Successfully installed faiss-cpu-1.9.0.post1 numpy-2.2.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "#pip installs\n",
    "%pip install opencv-python-headless transformers torch\n",
    "%pip install faiss-cpu\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "115cc7e2-944d-4fe3-ae87-0fc95f26a207",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: numpy 2.2.0\n",
      "Uninstalling numpy-2.2.0:\n",
      "  Successfully uninstalled numpy-2.2.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip uninstall numpy -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7dc7449a-6063-4dc2-9ca8-d339e6effaee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting accelerate>=0.26.0\n",
      "  Using cached accelerate-1.2.0-py3-none-any.whl (336 kB)\n",
      "Requirement already satisfied: torch>=1.10.0 in /usr/lib/python3/dist-packages (from accelerate>=0.26.0) (2.4.1)\n",
      "Requirement already satisfied: pyyaml in /usr/lib/python3/dist-packages (from accelerate>=0.26.0) (5.4.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/lib/python3/dist-packages (from accelerate>=0.26.0) (21.3)\n",
      "Requirement already satisfied: psutil in /usr/lib/python3/dist-packages (from accelerate>=0.26.0) (5.9.0)\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.17 in /usr/lib/python3/dist-packages (from accelerate>=0.26.0) (1.21.5)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in ./.local/lib/python3.10/site-packages (from accelerate>=0.26.0) (0.4.5)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.0 in ./.local/lib/python3.10/site-packages (from accelerate>=0.26.0) (0.26.5)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in ./.local/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate>=0.26.0) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/lib/python3/dist-packages (from huggingface-hub>=0.21.0->accelerate>=0.26.0) (4.9.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/lib/python3/dist-packages (from huggingface-hub>=0.21.0->accelerate>=0.26.0) (2024.3.1)\n",
      "Requirement already satisfied: filelock in /usr/lib/python3/dist-packages (from huggingface-hub>=0.21.0->accelerate>=0.26.0) (3.6.0)\n",
      "Requirement already satisfied: requests in /usr/lib/python3/dist-packages (from huggingface-hub>=0.21.0->accelerate>=0.26.0) (2.25.1)\n",
      "Installing collected packages: accelerate\n",
      "Successfully installed accelerate-1.2.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install 'accelerate>=0.26.0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6801295d-9329-4dfe-86fc-58e724e559a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting qwen-vl-utils\n",
      "  Using cached qwen_vl_utils-0.0.8-py3-none-any.whl (5.9 kB)\n",
      "Requirement already satisfied: av in ./.local/lib/python3.10/site-packages (from qwen-vl-utils) (14.0.1)\n",
      "Requirement already satisfied: pillow in /usr/lib/python3/dist-packages (from qwen-vl-utils) (9.0.1)\n",
      "Requirement already satisfied: packaging in /usr/lib/python3/dist-packages (from qwen-vl-utils) (21.3)\n",
      "Requirement already satisfied: requests in /usr/lib/python3/dist-packages (from qwen-vl-utils) (2.25.1)\n",
      "Installing collected packages: qwen-vl-utils\n",
      "Successfully installed qwen-vl-utils-0.0.8\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install qwen-vl-utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "258b6572-6ea7-4cf5-b1d2-955bc7fa320e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-12-10 14:49:09.492178: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-12-10 14:49:09.500588: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-12-10 14:49:09.504158: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import Qwen2VLForConditionalGeneration, AutoProcessor, AutoModelForCausalLM, AutoTokenizer, AutoModel\n",
    "from pathlib import Path\n",
    "\n",
    "from qwen_vl_utils import process_vision_info \n",
    "# from sentence_transformers import SentenceTransformer, util\n",
    "import faiss\n",
    "from pathlib import Path\n",
    "from transformers import pipeline\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec45d36-75ca-4f56-a7c2-86a6dc174761",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "05c2003e-0f28-4e8e-b4bc-aadb7e52d26c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_frames(video_path, frames_dir=\"./\", frame_rate=0.2):\n",
    "    \"\"\"Extract frames from a video at a specific rate and save to a directory.\"\"\"\n",
    "    video_name = Path(video_path).stem\n",
    "    video_frames_dir = Path(frames_dir) / video_name\n",
    "    video_frames_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    interval = int(fps / frame_rate)\n",
    "    frame_count = 0\n",
    "    saved_count = 0\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        if frame_count % interval == 0:\n",
    "            frame_path = video_frames_dir / f\"{saved_count:06d}.jpg\"\n",
    "            cv2.imwrite(str(frame_path), frame)\n",
    "            saved_count += 1\n",
    "        frame_count += 1\n",
    "\n",
    "    cap.release()\n",
    "    print(f\"Extracted {saved_count} frames from {video_path} to {video_frames_dir}\")\n",
    "    return video_frames_dir, saved_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9af11ef-0cc9-48b9-a5d6-f419e8607591",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_captions(video_frames_dir):\n",
    "    \"\"\"Generate captions for video chunks using the Qwen model in batches of 10 frames.\"\"\"\n",
    "    captions = []\n",
    "\n",
    "    # Sort the frames directory by file name to ensure proper order\n",
    "    frame_paths = sorted(video_frames_dir.glob(\"*.jpg\"))  # Adjust if your frames are stored with a different extension\n",
    "\n",
    "    # Define your prompt (as given)\n",
    "    prompt = (\n",
    "        \"Describe the scene at the self-checkout station, focusing on the customer's actions, especially with their hands. \"\n",
    "        \"Look for any signs of fraud, such as:\\n\"\n",
    "        \"- Not scanning items.\\n\"\n",
    "        \"- Concealing items.\\n\"\n",
    "        \"- Tampering with the checkout system.\\n\"\n",
    "        \"- Suspicious interactions or behaviors.\\n\\n\"\n",
    "        \"Highlight any actions that suggest theft or fraud.\\n\\n\"\n",
    "        \"Description:\"\n",
    "    )\n",
    "\n",
    "    # Process frames in batches of 10\n",
    "    for i in range(0, len(frame_paths), 10):\n",
    "        # Get the next batch of up to 10 frames\n",
    "        batch_paths = frame_paths[i:i + 10]\n",
    "\n",
    "        # Create the message structure for model input\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"video\", \"video\": [str(path) for path in batch_paths]},  # Pass image paths directly\n",
    "                    {\"type\": \"text\", \"text\": prompt},\n",
    "                ]\n",
    "            }\n",
    "        ]\n",
    "\n",
    "        # Process frames and prepare inputs for the model\n",
    "        text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "        image_inputs, video_inputs = process_vision_info(messages)\n",
    "\n",
    "        # Prepare the input for the model\n",
    "        inputs = processor(\n",
    "            text=[text],\n",
    "            images=image_inputs,\n",
    "            videos=video_inputs,\n",
    "            padding=True,\n",
    "            return_tensors=\"pt\"\n",
    "        ).to(\"cuda\")\n",
    "\n",
    "        # Inference: Generate output from the model\n",
    "        generated_ids = model.generate(**inputs, max_new_tokens=512)  # Customize max length as needed\n",
    "\n",
    "        # Trim the generated output to remove the input prompt\n",
    "        generated_ids_trimmed = [\n",
    "            out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "        ]\n",
    "        # Decode the output into human-readable captions\n",
    "        output_text = processor.batch_decode(generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "\n",
    "        # Print the generated caption for the batch\n",
    "        print(f\"Generated caption for batch starting with {batch_paths[0].stem}: {output_text[0]}\")\n",
    "\n",
    "        # Append the caption and its corresponding batch identifier (starting frame)\n",
    "        captions.append((batch_paths[0].stem, output_text[0]))\n",
    "\n",
    "    return captions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "00c420fc-0c43-406c-835a-eeb49c6dbe2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_path = \"./video/test.mp4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "069bd387-40ce-49b4-82ed-ac530e0e34fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 5/5 [00:01<00:00,  4.62it/s]\n",
      "/home/ubuntu/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1602: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  3.16it/s]\n"
     ]
    }
   ],
   "source": [
    "# Initialize the Qwen model and processor for video captioning\n",
    "model_name = \"Qwen/Qwen2-VL-7B-Instruct\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# model = Qwen2VLForConditionalGeneration.from_pretrained(model_name).to(device)\n",
    "model = Qwen2VLForConditionalGeneration.from_pretrained(model_name, torch_dtype=torch.bfloat16, attn_implementation=\"flash_attention\")\n",
    "processor = AutoProcessor.from_pretrained(model_name)\n",
    "\n",
    "embedding_model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "embedding_tokenizer = AutoTokenizer.from_pretrained(embedding_model_name)\n",
    "embedding_model = AutoModel.from_pretrained(embedding_model_name).to(device)\n",
    "\n",
    "llm_model_name = \"meta-llama/Llama-2-13b-chat-hf\"\n",
    "llm_tokenizer = AutoTokenizer.from_pretrained(llm_model_name)\n",
    "llm_model = AutoModelForCausalLM.from_pretrained(llm_model_name).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "41bfc60d-ad4a-4268-bd76-32f99677308a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Qwen2VLForConditionalGeneration(\n",
       "  (visual): Qwen2VisionTransformerPretrainedModel(\n",
       "    (patch_embed): PatchEmbed(\n",
       "      (proj): Conv3d(3, 1280, kernel_size=(2, 14, 14), stride=(2, 14, 14), bias=False)\n",
       "    )\n",
       "    (rotary_pos_emb): VisionRotaryEmbedding()\n",
       "    (blocks): ModuleList(\n",
       "      (0-31): 32 x Qwen2VLVisionBlock(\n",
       "        (norm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): VisionSdpaAttention(\n",
       "          (qkv): Linear(in_features=1280, out_features=3840, bias=True)\n",
       "          (proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (mlp): VisionMlp(\n",
       "          (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "          (act): QuickGELUActivation()\n",
       "          (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (merger): PatchMerger(\n",
       "      (ln_q): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Sequential(\n",
       "        (0): Linear(in_features=5120, out_features=5120, bias=True)\n",
       "        (1): GELU(approximate='none')\n",
       "        (2): Linear(in_features=5120, out_features=3584, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (model): Qwen2VLModel(\n",
       "    (embed_tokens): Embedding(152064, 3584)\n",
       "    (layers): ModuleList(\n",
       "      (0-27): 28 x Qwen2VLDecoderLayer(\n",
       "        (self_attn): Qwen2VLSdpaAttention(\n",
       "          (q_proj): Linear(in_features=3584, out_features=3584, bias=True)\n",
       "          (k_proj): Linear(in_features=3584, out_features=512, bias=True)\n",
       "          (v_proj): Linear(in_features=3584, out_features=512, bias=True)\n",
       "          (o_proj): Linear(in_features=3584, out_features=3584, bias=False)\n",
       "          (rotary_emb): Qwen2RotaryEmbedding()\n",
       "        )\n",
       "        (mlp): Qwen2MLP(\n",
       "          (gate_proj): Linear(in_features=3584, out_features=18944, bias=False)\n",
       "          (up_proj): Linear(in_features=3584, out_features=18944, bias=False)\n",
       "          (down_proj): Linear(in_features=18944, out_features=3584, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
       "        (post_attention_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=3584, out_features=152064, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "28ef1234-7968-49f5-b74c-41d29ebcc0a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_texts(texts, tokenizer, model):\n",
    "    \"\"\"\n",
    "    Encode a list of texts into embeddings using a Hugging Face model.\n",
    "\n",
    "    Args:\n",
    "        texts (list of str): List of input texts to encode.\n",
    "        tokenizer: Hugging Face tokenizer.\n",
    "        model: Hugging Face model.\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: Embeddings for the input texts.\n",
    "    \"\"\"\n",
    "    inputs = tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "    inputs = {key: value.to(model.device) for key, value in inputs.items()}\n",
    "\n",
    "    # Get embeddings from the model\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        embeddings = outputs.last_hidden_state.mean(dim=1).cpu().numpy()  # Use mean pooling\n",
    "    return embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "62bc4efc-ef7a-4896-831f-d2765e7a7c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_faiss_index(captions):\n",
    "    \"\"\"\n",
    "    Create a FAISS index to store and search embeddings for caption similarity.\n",
    "\n",
    "    Args:\n",
    "        captions (list of tuples): Each tuple contains (frame_id, caption_text), where 'caption_text' is a caption\n",
    "                                   string to be embedded and indexed.\n",
    "\n",
    "    Returns:\n",
    "        index (faiss.IndexFlatL2): A FAISS index with stored embeddings that allows similarity searches.\n",
    "        captions (list of tuples): The original captions list, which maintains the frame and caption structure.\n",
    "    \"\"\"\n",
    "    # Extract the caption text from each tuple (frame_id, caption_text) for embedding\n",
    "    captions_text = [caption[1] for caption in captions]\n",
    "\n",
    "    # Generate embeddings for each caption using the Hugging Face embedding model\n",
    "    embeddings = encode_texts(captions_text, embedding_tokenizer, embedding_model)\n",
    "\n",
    "    # Initialize a FAISS index with L2 (Euclidean) distance metric using the dimensionality of the embeddings\n",
    "    index = faiss.IndexFlatL2(embeddings.shape[1])\n",
    "\n",
    "    # Add the embeddings to the FAISS index, enabling fast similarity search on these vectors\n",
    "    index.add(embeddings)\n",
    "\n",
    "    # Return both the FAISS index and the original captions list for later retrieval and reference\n",
    "    return index, captions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "698c058d-8aeb-4fc0-98ac-41fdc37d6957",
   "metadata": {},
   "outputs": [],
   "source": [
    "def refine_captions(index, captions, k=5):\n",
    "    \"\"\"\n",
    "    Refine captions by finding the most semantically similar captions within a neighborhood using FAISS.\n",
    "\n",
    "    Args:\n",
    "        index (faiss.IndexFlatL2): A FAISS index containing embeddings for similarity search.\n",
    "        captions (list of tuples): Original captions with structure (frame_id, caption_text).\n",
    "        k (int): Number of nearest neighbors to consider.\n",
    "\n",
    "    Returns:\n",
    "        list of tuples: Refined captions with the same structure as the input.\n",
    "    \"\"\"\n",
    "    # Extract the caption text from each tuple for embedding\n",
    "    captions_text = [caption[1] for caption in captions]\n",
    "\n",
    "    # Generate embeddings for the captions using the Hugging Face embedding model\n",
    "    embeddings = encode_texts(captions_text, embedding_tokenizer, embedding_model)\n",
    "\n",
    "    # Search for the k nearest neighbors in the FAISS index\n",
    "    _, neighbors = index.search(embeddings, k)\n",
    "\n",
    "    # Refine captions based on the most frequent neighbor caption\n",
    "    refined_captions = []\n",
    "    for idx, neighbor_indices in enumerate(neighbors):\n",
    "        neighbor_texts = [captions[neighbor][1] for neighbor in neighbor_indices]\n",
    "        most_frequent_caption = max(set(neighbor_texts), key=neighbor_texts.count)\n",
    "        refined_captions.append((captions[idx][0], most_frequent_caption))\n",
    "    \n",
    "    return refined_captions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d38df936-4b36-4136-b408-fe5f21f6b494",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def anomaly_score(summaries):\n",
    "    \"\"\"Assign anomaly scores to each summary using LLM based on predefined behavior patterns.\"\"\"\n",
    "    scores = []\n",
    "\n",
    "    for summary in summaries:\n",
    "        # System prompt to guide the model\n",
    "        system_prompt = (\n",
    "            \"You are an expert in detecting suspicious behavior in retail environments like shopping malls.\\n\"\n",
    "            \"Your task is to evaluate descriptions of customer actions and identify potential signs of theft or suspicious behavior. \"\n",
    "            \"Please follow these steps:\"\n",
    "            \"1. Explain your thought process in detail, outlining any behaviors or actions that led you to conclude whether theft or suspicious behavior might be happening. DO NOT MENTION ANY SCORE OR NUMBER IN YOUR EXPLANATION\"\n",
    "            \"2. Based on your reasoning, provide a floating point number that lies between 0 and 1, where: \"\n",
    "            \"   - 0 means no signs of theft, and \"\n",
    "            \"   - 1 means a high likelihood of theft.\"\n",
    "        )\n",
    "\n",
    "        # Create the full prompt with system instruction\n",
    "        prompt = (\n",
    "            system_prompt + \"\\n\"\n",
    "            \"Scene Description:\" + summary[1] + \"\\n\\n\"\n",
    "            \"THEFT SCORE:\"\n",
    "        )\n",
    "\n",
    "        # Encode the prompt for the T5 model\n",
    "        inputs = llm_tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "        # Generate a response from the model\n",
    "        output = llm_model.generate(\n",
    "            **inputs,\n",
    "            max_length=1000,  # Ensure the model doesn't generate overly long outputs\n",
    "            eos_token_id=llm_tokenizer.eos_token_id,\n",
    "        )\n",
    "\n",
    "        # Decode the response\n",
    "        response = llm_tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "        # Remove the initial prompt from the result\n",
    "        if \"THEFT SCORE:\" in response:\n",
    "            response = response.split(\"THEFT SCORE:\")[1].strip()\n",
    "        else:\n",
    "            response = response.strip()\n",
    "\n",
    "        print(\"Response: \", response)\n",
    "\n",
    "        # Use regex to extract a floating point number between 0 and 1, or integers like 0 or 1\n",
    "        match = re.search(r'\\b0?(\\.\\d+)?\\b', response)\n",
    "        if match:\n",
    "            score = float(match.group())\n",
    "            print(\"Extracted score is: \", score)\n",
    "        else:\n",
    "            score = None\n",
    "            print(\"No score found in response\")\n",
    "\n",
    "        # Append the score to the list\n",
    "        scores.append(score)\n",
    "\n",
    "    return scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "52061a6f-87ab-4c91-8d62-629936f16975",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_anomoly(scores, threshold=0.7):\n",
    "    # Check if any frame has a score above the threshold\n",
    "    is_anomalous = any(score >= threshold for score in scores)\n",
    "\n",
    "    # Return \"Yes\" if any anomaly is detected, otherwise \"No\"\n",
    "    return \"Yes\" if is_anomalous else \"No\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7c2ed0ce-e6f0-4fa6-a0bb-e7d3b4e64205",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_eval(video_dir, output_csv=\"evaluation_results.csv\", threshold=0.6):\n",
    "    \"\"\"\n",
    "    Evaluate the LAVAD flow for multiple videos and generate a CSV result.\n",
    "    If the CSV file exists, it appends new results; otherwise, it creates a new file.\n",
    "\n",
    "    Args:\n",
    "        video_dir (str): Directory containing video files to process.\n",
    "        output_csv (str): Path to save the evaluation results as a CSV file.\n",
    "        threshold (float): Anomaly detection threshold.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "\n",
    "    # Check if the CSV file exists\n",
    "    if not os.path.exists(output_csv):\n",
    "        # Create a new CSV file with appropriate headers\n",
    "        pd.DataFrame(columns=[\n",
    "            \"video_name\", \"frame_count\", \"anomaly_detected\",\n",
    "            \"anomaly_scores\", \"average_score\", \"error_message\"\n",
    "        ]).to_csv(output_csv, index=False)\n",
    "\n",
    "    # Load existing results to avoid duplicate processing\n",
    "    existing_results = pd.read_csv(output_csv)\n",
    "    processed_videos = existing_results[\"video_name\"].tolist()\n",
    "\n",
    "    # Initialize evaluation results\n",
    "    results = []\n",
    "\n",
    "    # Iterate through each video file in the directory\n",
    "    for video_file in Path(video_dir).glob(\"*.mp4\"):\n",
    "        if video_file.name in processed_videos:\n",
    "            print(f\"Skipping already processed video: {video_file.name}\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            print(f\"Processing video: {video_file.name}\")\n",
    "\n",
    "            # Step 1: Extract frames\n",
    "            print(\"Extracting frames .................................................................\\n\")\n",
    "            video_frames_dir, chunk_count = extract_frames(video_path)\n",
    "\n",
    "            # Step 2: Generate\n",
    "            print(\"Generating captions....................................................................\\n\")\n",
    "            captions = generate_captions(Path(video_frames_dir))\n",
    "\n",
    "            # Step 3: Create and refine captions using FAISS\n",
    "            print(\"cleaning caption .....................................................................\\n\")\n",
    "            index, raw_captions = create_faiss_index(captions)\n",
    "            refined_captions = refine_captions(index, raw_captions)\n",
    "\n",
    "            # Step 4: Assign anomaly scores\n",
    "            print(\"Creating scores.......................................................................\\n\")\n",
    "            scores = anomaly_score(refined_captions)\n",
    "\n",
    "            # Step 5: Determine if the video contains anomalies\n",
    "            print(\"Detecting anomalies....................................................................\\n\")\n",
    "            anomaly_detected = is_anomoly(scores, threshold)\n",
    "\n",
    "            # Log the result\n",
    "            results.append({\n",
    "                \"video_name\": video_file.name,\n",
    "                \"frame_count\": frame_count,\n",
    "                \"anomaly_detected\": anomaly_detected,\n",
    "                \"anomaly_scores\": scores,\n",
    "                \"average_score\": sum(scores) / len(scores) if scores else 0,\n",
    "                \"error_message\": \"\"\n",
    "            })\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing video {video_file.name}: {e}\")\n",
    "            results.append({\n",
    "                \"video_name\": video_file.name,\n",
    "                \"frame_count\": 0,\n",
    "                \"anomaly_detected\": \"Error\",\n",
    "                \"anomaly_scores\": [],\n",
    "                \"average_score\": 0,\n",
    "                \"error_message\": str(e)\n",
    "            })\n",
    "\n",
    "    # Append new results to the CSV\n",
    "    if results:\n",
    "        new_results_df = pd.DataFrame(results)\n",
    "        new_results_df.to_csv(output_csv, mode='a', header=False, index=False)\n",
    "        print(f\"Updated evaluation results saved to {output_csv}\")\n",
    "    else:\n",
    "        print(\"No new videos processed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "251f4c75-995c-4345-8e65-44aed6e24b11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing video: test.mp4\n",
      "Extracting frames .................................................................\n",
      "\n",
      "Extracted 13 frames from ./video/test.mp4 to test\n",
      "Generating captions....................................................................\n",
      "\n",
      "Error processing video test.mp4: apply_chat_template requires jinja2>=3.1.0 to be installed. Your version is 3.0.3.\n",
      "Updated evaluation results saved to evaluation_results.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "video_directory = \"./video\"\n",
    "run_eval(video_directory, output_csv=\"evaluation_results.csv\", threshold=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b436364d-0e8a-4e84-bf97-b0486e338eb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip uninstall jinja2>=3.1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f843545-e9ee-48a1-81e0-30750ddf8ade",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting git+https://github.com/huggingface/transformers@21fac7abba2a37fae86106f87fcf9974fd1e3830\n",
      "  Cloning https://github.com/huggingface/transformers (to revision 21fac7abba2a37fae86106f87fcf9974fd1e3830) to /tmp/pip-req-build-o429x9nw\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers /tmp/pip-req-build-o429x9nw\n",
      "  Running command git rev-parse -q --verify 'sha^21fac7abba2a37fae86106f87fcf9974fd1e3830'\n",
      "  Running command git fetch -q https://github.com/huggingface/transformers 21fac7abba2a37fae86106f87fcf9974fd1e3830\n",
      "  Running command git checkout -q 21fac7abba2a37fae86106f87fcf9974fd1e3830\n",
      "  Resolved https://github.com/huggingface/transformers to commit 21fac7abba2a37fae86106f87fcf9974fd1e3830\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: accelerate in ./.local/lib/python3.10/site-packages (1.2.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in ./.local/lib/python3.10/site-packages (from transformers==4.45.0.dev0) (0.4.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in ./.local/lib/python3.10/site-packages (from transformers==4.45.0.dev0) (4.67.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/lib/python3/dist-packages (from transformers==4.45.0.dev0) (21.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/lib/python3/dist-packages (from transformers==4.45.0.dev0) (1.21.5)\n",
      "Collecting tokenizers<0.20,>=0.19\n",
      "  Downloading tokenizers-0.19.1-cp310-cp310-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (3.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in /usr/lib/python3/dist-packages (from transformers==4.45.0.dev0) (3.6.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in ./.local/lib/python3.10/site-packages (from transformers==4.45.0.dev0) (0.26.5)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./.local/lib/python3.10/site-packages (from transformers==4.45.0.dev0) (2024.11.6)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/lib/python3/dist-packages (from transformers==4.45.0.dev0) (5.4.1)\n",
      "Requirement already satisfied: requests in /usr/lib/python3/dist-packages (from transformers==4.45.0.dev0) (2.25.1)\n",
      "Requirement already satisfied: torch>=1.10.0 in /usr/lib/python3/dist-packages (from accelerate) (2.4.1)\n",
      "Requirement already satisfied: psutil in /usr/lib/python3/dist-packages (from accelerate) (5.9.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/lib/python3/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.45.0.dev0) (2024.3.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/lib/python3/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.45.0.dev0) (4.9.0)\n",
      "Building wheels for collected packages: transformers\n",
      "  Building wheel for transformers (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for transformers: filename=transformers-4.45.0.dev0-py3-none-any.whl size=9633202 sha256=be221cc5a59ed8ea7c9b45036a32e2ce35d1c34f9d182ad2abb6de86c27ad0b4\n",
      "  Stored in directory: /home/ubuntu/.cache/pip/wheels/28/41/24/cb082d58fae00708aaa217e3e6162526ba4d1f24f27e06b0bf\n",
      "Successfully built transformers\n",
      "Installing collected packages: tokenizers, transformers\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.21.0\n",
      "    Uninstalling tokenizers-0.21.0:\n",
      "      Successfully uninstalled tokenizers-0.21.0\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.47.0\n",
      "    Uninstalling transformers-4.47.0:\n",
      "      Successfully uninstalled transformers-4.47.0\n",
      "Successfully installed tokenizers-0.19.1 transformers-4.45.0.dev0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install git+https://github.com/huggingface/transformers@21fac7abba2a37fae86106f87fcf9974fd1e3830 accelerate flash_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd50bc17-5fc8-4222-bd74-3acfb801e20c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
